{"cells":[{"cell_type":"markdown","metadata":{"id":"x8Z2YbQZwNV7"},"source":["#**Week14_복습과제_우정연**"]},{"cell_type":"markdown","metadata":{"id":"0iuIH1nQwW-r"},"source":["##**8.4 텍스트 분류 실습 - 20 뉴스그룹 분류**"]},{"cell_type":"markdown","metadata":{"id":"Lx8E1FYzxAh0"},"source":["- 텍스트 분류: 특정 문서의 분류를 학습 데이터를 통해 학습해 모델을 생성한 뒤 이 학습 모델을 이용해 다른 문서의 분류를 예측하는 것\n","- 사이킷런의 `fetch_20newsgroup() API`: 뉴스그룹의 분류를 수행해 볼 수 있는 예제 데이터를 제공\n","- 희소행렬에 분류를 효과적으로 잘 처리할 수 있는 알고리즘: 로지스틱 회귀, 선형 서포트 벡터 머신, 나이브 베이즈 등\n","- 로지스틱 회귀를 이용한 분류 수행"]},{"cell_type":"markdown","metadata":{"id":"44c9U5mqyZnI"},"source":["1. 텍스트 정규화한 뒤 피처 벡터화 적용\n","2. 적합한 머신러닝 알고리즘을 적용해 분류를 학습/예측/평가"]},{"cell_type":"markdown","metadata":{"id":"3wOvoX5Aykg_"},"source":["이번 예제\n","- 카운트 기반과 TF-IDF 기반의 벡터화를 차례로 적용해 예측 성능 비교\n","- 피처 벡터화를 위한 파라미터와 GridSearchCV 기반의 하이퍼 파라미터 튜닝\n","- 사이킷런의 Pipeline 객체를 통해 피처 벡터화 파라미터와 GridSearchCV 기반의 하이퍼 파라미터 튜닝을 한꺼번에 수행하는 방법"]},{"cell_type":"markdown","metadata":{"id":"vlP0kjDiy_4F"},"source":["###**[텍스트 정규화]**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":863,"status":"ok","timestamp":1735292455414,"user":{"displayName":"우정연","userId":"02785637225882896926"},"user_tz":-540},"id":"ySTeXS8OwNIW","outputId":"850f0bff-d826-4b3f-8174-405947275227"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","news_data = fetch_20newsgroups(subset = 'all', random_state = 156)\n","\n","print(news_data.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":599,"status":"ok","timestamp":1735292456011,"user":{"displayName":"우정연","userId":"02785637225882896926"},"user_tz":-540},"id":"xfncf3qXwGx1","outputId":"543721e1-1388-42b0-9c4c-7ae3e9042013"},"outputs":[{"name":"stdout","output_type":"stream","text":["target 클래스의 값과 분포도 \n"," 0     799\n","1     973\n","2     985\n","3     982\n","4     963\n","5     988\n","6     975\n","7     990\n","8     996\n","9     994\n","10    999\n","11    991\n","12    984\n","13    990\n","14    987\n","15    997\n","16    910\n","17    940\n","18    775\n","19    628\n","Name: count, dtype: int64\n","target 클래스의 이름들 \n"," ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"]}],"source":["import pandas as pd\n","\n","print('target 클래스의 값과 분포도 \\n', pd.Series(news_data.target).value_counts().sort_index())\n","print('target 클래스의 이름들 \\n', news_data.target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1735292456011,"user":{"displayName":"우정연","userId":"02785637225882896926"},"user_tz":-540},"id":"3GNCNkwP0Pfh","outputId":"51b2956d-35cf-4b8a-d5e3-77962a8270a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n","Subject: Re: Observation re: helmets\n","Organization: Sun Microsystems, RTP, NC\n","Lines: 21\n","Distribution: world\n","Reply-To: egreen@east.sun.com\n","NNTP-Posting-Host: laser.east.sun.com\n","\n","In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n","> \n","> The question for the day is re: passenger helmets, if you don't know for \n",">certain who's gonna ride with you (like say you meet them at a .... church \n",">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",">pick up another shoei in my size to have a backup helmet (XL), or should I \n",">maybe get an inexpensive one of a smaller size to accomodate my likely \n",">passenger? \n","\n","If your primary concern is protecting the passenger in the event of a\n","crash, have him or her fitted for a helmet that is their size.  If your\n","primary concern is complying with stupid helmet laws, carry a real big\n","spare (you can put a big or small head in a big helmet, but not in a\n","small one).\n","\n","---\n","Ed Green, former Ninjaite |I was drinking last night with a biker,\n","  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n","DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n"," (The Grateful Dead) -->  |It seemed like the least I could do...\n","\n","\n"]}],"source":["print(news_data.data[0])"]},{"cell_type":"markdown","metadata":{"id":"GnkrqH_01Zfd"},"source":["- 내용을 제외하고 제목 등 다른 정보는 제거\n","  - 제목과 소속, 이메일 주소 등의 헤더와 푸터 정보들은 뉴스그룹 분류의 Target 클래스 값과 유사한 데이터를 가지고 있는 경우가 많기 때문\n","  - 이 피처들을 포함하면 웬만한 ML 알고리즘을 적용해도 상당히 높은 예측 성능을 나타냄\n","  - 순수한 텍스트만으로 구성된 기사 내용으로 어떤 뉴스그룹에 속하는지 분류할 것\n","- remove 파라미터를 이용해 제거"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3597,"status":"ok","timestamp":1735292459606,"user":{"displayName":"우정연","userId":"02785637225882896926"},"user_tz":-540},"id":"XGUYTZj20RSE","outputId":"28b54982-9bc9-47fc-a5a4-0bbd88ab6f24"},"outputs":[{"name":"stdout","output_type":"stream","text":["학습 데이터 크기 11314, 테스트 데이터 크기 7532\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","# subset='train'으로 학습용 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n","train_news = fetch_20newsgroups(subset = 'train', remove = ('headers', 'footers', 'quotes'),\n","                   random_state = 156)\n","X_train = train_news.data\n","y_train = train_news.target\n","\n","# subset='test'로 테스트 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n","test_news = fetch_20newsgroups(subset = 'test', remove = ('headers', 'footers', 'quotes'),\n","                               random_state = 156)\n","X_test = test_news.data\n","y_test = test_news.target\n","print('학습 데이터 크기 {0}, 테스트 데이터 크기 {1}'.format(len(train_news.data),\n","                                             len(test_news.data)))"]},{"cell_type":"markdown","metadata":{"id":"2CwwWADk0PBJ"},"source":["###**[피처 벡터화 변환과 머신러닝 모델 학습/예측/평가]**"]},{"cell_type":"markdown","metadata":{"id":"UV3kiRIH4bxG"},"source":["- CountVectorizer를 이용해 학습 데이터의 텍스트를 피처 벡터화\n","  - 유의할 점\n","    - 테스트 데이터에서 CountVectorizer를 적용할 때는 반드시 학습 데이터를 이용해 `fit()`이 수행된 CountVectorizer 객체를 이용해 테스트 데이터를 변환해야 한다는 것\n","    - 그래야만 학습 시 설정된 CountVectorizer의 피처 개수와 테스트 데이터를 CounterVectorizer로 변환할 피처 개수가 같아짐\n","    - 테스트 데이터의 피처 벡터화는 학습 데이터에 사용된 CountVectorizer 객체 변수인 `cnt_vect.transform()`을 이용해 변환\n","    - 테스트 데이터의 피처 벡터화 시 `fit_transfrom()`을 사용하면 안됨\n","      - `CounterVectorizer.fit_transform(테스트 데이터)`을 테스트 데이터 세트에 적용하면 테스트 데이터 기반으로 다시 CounterVectorizer가 `fit()`을 수행하고 `transform()`하기 때문에 학습 시 사용된 피처 개수와 예측 시 사용할 피처 개수가 달라짐"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5442,"status":"ok","timestamp":1735292465045,"user":{"displayName":"우정연","userId":"02785637225882896926"},"user_tz":-540},"id":"GKe95lBX7GrZ","outputId":"8c7dde35-fd74-4ebd-a6b7-d6690105f175"},"outputs":[{"name":"stdout","output_type":"stream","text":["학습 데이터 텍스트의 CountVectorizer Shape: (11314, 101631)\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Count Vectorization으로 피처 벡터화 변환 수행\n","cnt_vect = CountVectorizer()\n","cnt_vect.fit(X_train)\n","X_train_cnt_vect = cnt_vect.transform(X_train)\n","\n","# 학습 데이터로 fit()된 CountVectorizer를 이요해 테스트 데이터를 피터 벡터화 변환 수행\n","X_test_cnt_vect = cnt_vect.transform(X_test)\n","\n","print('학습 데이터 텍스트의 CountVectorizer Shape:', X_train_cnt_vect.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55232,"status":"ok","timestamp":1735292520274,"user":{"displayName":"우정연","userId":"02785637225882896926"},"user_tz":-540},"id":"VhQlWfH37un8","outputId":"4d3db4a8-376e-4752-e8ed-31178f69c998"},"outputs":[{"name":"stdout","output_type":"stream","text":["CountVectorized Logistic Regression의 예측 정확도는 0.603\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# LogisticRegression을 이용해 학습/예측/평가 수행\n","lr_clf = LogisticRegression()\n","lr_clf.fit(X_train_cnt_vect, y_train)\n","pred = lr_clf.predict(X_test_cnt_vect)\n","print('CountVectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35564,"status":"ok","timestamp":1735292555835,"user":{"displayName":"우정연","userId":"02785637225882896926"},"user_tz":-540},"id":"vrtiHZnpSyrr","outputId":"a4dff77e-2a6a-4fd4-bd93-c52f6fbfd9ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF-IDF Logistic Regression의 예측 정확도는 0.674\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# TF-IDF 벡터화를 적용해 학습 데이터 세트와 테스트 데이터 세트 변환\n","tfidf_vect = TfidfVectorizer()\n","tfidf_vect.fit(X_train)\n","X_train_tfidf_vect = tfidf_vect.transform(X_train)\n","X_test_tfidf_vect = tfidf_vect.transform(X_test)\n","\n","# LogisticRegression을 이용해 학습/예측/평가 수행\n","lr_clf = LogisticRegression()\n","lr_clf.fit(X_train_tfidf_vect, y_train)\n","pred = lr_clf.predict(X_test_tfidf_vect)\n","print('TF-IDF Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"]},{"cell_type":"markdown","metadata":{"id":"A05ziLLpUO84"},"source":["- TF-IDF가 단순 카운트 기반보다 높은 예측 정확도를 제공함\n","- 일반적으로 문서 내에 텍스트가 많고, 많은 문서를 가지는 텍스트 분석에서 카운트 벡터화보다는 TF-IDF 벡터화가 좋은 예측 결과를 도출함"]},{"cell_type":"markdown","metadata":{"id":"J662e1hiUbpm"},"source":["- 텍스트 분석에서 ML 모델의 성능을 향상시키는 중요한 2가지 방법\n","  - 최적의 ML 알고리즘 선택하기\n","  - 최상의 피처 전처리를 수행하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171191,"status":"ok","timestamp":1735292727023,"user":{"displayName":"우정연","userId":"02785637225882896926"},"user_tz":-540},"id":"SN7wNsW-UnK_","outputId":"90183045-fd92-4c2d-df62-c0e4cf4f29a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF-IDF Vectorized Logistic Regression의 예측 정확도는 0.692\n"]}],"source":["# stop words 필터링을 추가하고 ngram을 기본(1, 1)에서 (1, 2)로 변경해 피처 벡터화 적용\n","tfidf_vect = TfidfVectorizer(stop_words = 'english', ngram_range = (1,2), max_df = 300)\n","tfidf_vect.fit(X_train)\n","X_train_tfidf_vect = tfidf_vect.transform(X_train)\n","X_test_tfidf_vect = tfidf_vect.transform(X_test)\n","\n","lr_clf = LogisticRegression()\n","lr_clf.fit(X_train_tfidf_vect, y_train)\n","pred = lr_clf.predict(X_test_tfidf_vect)\n","print('TF-IDF Vectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0uj-U0m-VaRd","outputId":"09d8d18a-bd2c-4f5a-cab9-edcecd97922e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 5 candidates, totalling 15 fits\n","Logistic Regression best C parameter:  {'C': 10}\n","TF-IDF Vectorized Logistic Regression의 예측 정확도는 0.701\n"]}],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# 최적 C 값 도출 튜닝 수행. CV는 3폴드 세트로 설정\n","params = { 'C':[0.01, 0.1, 1, 5, 10]}\n","grid_cv_lr = GridSearchCV(lr_clf, param_grid = params, cv = 3, scoring = 'accuracy', verbose = 1)\n","grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n","print('Logistic Regression best C parameter: ', grid_cv_lr.best_params_)\n","\n","# 최적 C 값으로 학습된 grid_cv로 예측 및 정확도 평가\n","pred = grid_cv_lr.predict(X_test_tfidf_vect)\n","print('TF-IDF Vectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"]},{"cell_type":"markdown","metadata":{"id":"-uW42yGaWObz"},"source":["- 로지스틱 회귀의 C가 10일 때 GridSearchCV의 교차 검증 테스트 세트에서 가장 좋은 예측 성능"]},{"cell_type":"markdown","metadata":{"id":"3NJ9JEVoWW4J"},"source":["###**[사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합]**"]},{"cell_type":"markdown","metadata":{"id":"IsejNhDRWe3G"},"source":["- 사이킷런의 Pipeline 클래스를 이용\n","  - 피처 벡터화와 ML 알고리즘 학습/예측을 위한 코드 작성을 한 번에 진행 가능\n","  - 데이터 전처리와 ML 학습과정을 통일된 API 기반에서 처리할 수 있어 더 직관적인 ML 모델 코드를 생성할 수 있음\n","  - 대용량 데이터의 피처 벡터화 결과를 별도 데이터로 저장하지 않고 스트림 기반에서 바로 머신러닝 알고리즘의 데이터로 입력할 수 있기 때문에 수행 시간을 절약할 수 있음.\n","  - 텍스트 기반의 피처 벡터화뿐만 아니라 모든 데이터 전처리 작업과 Estimator를 결합 가능\n","    - 예) 스케일링 또는 벡터 정규화, PCA 등의 변환 작업과 분류, 회귀 등의 Estimator를 한 번에 결합하는 것"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nV8Cd_JkXZ11","executionInfo":{"status":"ok","timestamp":1735296836649,"user_tz":-540,"elapsed":344229,"user":{"displayName":"우정연","userId":"02785637225882896926"}},"outputId":"3b24d562-2354-4d05-c17e-17f124bdd603"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pipeline을 통한 Logistic Regression의 예측 정확도는 0.701\n"]}],"source":["# TfidfVectorizer 객체 -> tfidf_vect 객체 변수 명\n","# LogisticRegression 객체 -> lr_clf 객체 변수 명\n","# 두 객체를 파이프라인으로 연결하는 Pipeline 객체 pipeline 생성\n","\n","from sklearn.pipeline import Pipeline\n","\n","pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words = 'english', ngram_range = (1,2), max_df = 300)),\n","                     ('lr_clf', LogisticRegression(C = 10))])\n","\n","# 별도의 TfidfVectorizer 객체의 fit(), transform()과 LogisticRegression의 fit(), predict()가 필요 없음\n","# pipeline의 fit()과 predict()만으로 한꺼번에 피처 벡터화와 ML 학습/예측이 가능\n","\n","pipeline.fit(X_train, y_train)\n","pred = pipeline.predict(X_test)\n","print('Pipeline을 통한 Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))\n","\n"]},{"cell_type":"markdown","source":["- GridSearchCV 클래스의 생성 파라미터로 Pipeline을 입력해 Pipeline 기반에서도 하이퍼 파라미터 튜닝을 GridSearchCV 방식으로 진행할 수 있게 지원함\n","  - 피처 벡터화를 위한 파라미터와 ML 알고리즘의 하이퍼 파라미터를 모두 한 번에 GridSearchCV를 이용해 최적화할 수 있음"],"metadata":{"id":"euBPynyMvico"}},{"cell_type":"markdown","source":["- GridSearchCV에 Pipeline을 입력하면서 TfidfVectorizer의 파라미터와 Logistic Regression의 하이퍼 파라미터를 함께 최적화하는 예시\n","  - GridSearchCv에 Estimator가 아닌 Pipeline을 입력할 경우, param_grid의 입력 값 설정이 기존과 약간 다름\n","  - Key 값: `tfidf_vect__ngram_range`와 같이 하이퍼 파라미터 명이 객체 변수명과 결합되어 제공됨\n","    - Pipeline을 GridSearchCV에 인자로 입력하면 GridSearchCV는 Pipeline을 구성하는 피처 벡터화 객체의 파라미터와 Estimator 객체의 하이처 파라미터를 각각 구별할 수 있어야 함 -> 개별 객체 명과 파라미터명/하이퍼 파라미터명을 결합해 Key 값으로 할당하는 것\n","- Pipeline + GridSearchCV 적용 시 유의할 점\n","  - 모든 파라미터를 최적화하려면 튜닝 시간이 너무 오래 걸림\n","  - 파라미터와 GridSearchCV 하이퍼 파라미터를 합치면 최적화를 위한 너무 많은 경우의 수가 발생하기 쉬움"],"metadata":{"id":"1NkdnhKdv-WU"}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\n","\n","pipeline = Pipeline([\n","    ('tfidf_vect', TfidfVectorizer(stop_words = 'english')),\n","    ('lr_clf', LogisticRegression())\n","])\n","\n","# Pipeline에 기술된 각각의 객체 변수에 언더바(_) 2개를 연달아 붙여 GridSearchCV에 사용될\n","# 파라미터/하이퍼 파라미터 이름과 값을 설정\n","params = { 'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],\n","          'tfidf_vect__max_df': [100, 300, 700],\n","           'lr_clf__C': [1, 5, 10]}\n","\n","# GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력\n","grid_cv_pipe = GridSearchCV(pipeline, param_grid = params, cv = 3, scoring = 'accuracy', verbose = 1)\n","grid_cv_pipe.fit(X_train, y_train)\n","print(grid_cv_pipe.best_params_, grid_cv_pipe.best_score_)\n","\n","pred = grid_cv_pipe.predict(X_test)\n","print('Pipeline을 통한 Logistic Regressiondml dPcmr wjdghkrehsms {0:.3f}'.format(\n","    accuracy_score(y_test, pred)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6HoUmfi3Hm3","executionInfo":{"status":"ok","timestamp":1735311278549,"user_tz":-540,"elapsed":12302476,"user":{"displayName":"우정연","userId":"02785637225882896926"}},"outputId":"de1cb2b3-85c5-43f1-a878-2ed4729d6a01"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 27 candidates, totalling 81 fits\n","{'lr_clf__C': 10, 'tfidf_vect__max_df': 300, 'tfidf_vect__ngram_range': (1, 2)} 0.7533152393023013\n","Pipeline을 통한 Logistic Regressiondml dPcmr wjdghkrehsms 0.701\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPiHt+OJwxa7Tr2Jk3rpKLd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}